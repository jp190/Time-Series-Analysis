{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch training of data (15 + 10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.Modify the implementation of the network to leverage the RNN subclass of module torch.nn, which readily incorporates support for batch training. Set the hidden state size to 128 and train the network through five epochs with a batch size equal to the total number of samples. Note that, since the data samples are of different lengths, you will need to pad the length of the samples to a unique sequence length (e.g., at least the length of the longest sequence) in order to be able to feed the batch to the network. This is because RNN expects the input to be a tensor of shape (seq_len, batch, input_size). You can either manually pad with 0s, or you can use built-in functions such as torch.nn.utils.rnn.pad_sequence to perform the padding. Report the accuracy number yielded by this approach on the full training set. (15 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Hyper Parameters\n",
    "NEPOCHS = 5\n",
    "BATCH_SIZE = 20074\n",
    "TIME_STEP = 57          \n",
    "INPUT_SIZE = 57         \n",
    "LR = 0.01               # learning rate\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the names dictionary, a list of names per language\n",
    "# dictionary keys are languages, values are names\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def findName(dict, name):\n",
    "    keys = dict.keys()\n",
    "    for key in keys:\n",
    "        if name in dict[key]:\n",
    "            return key\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "def get_key(val): \n",
    "    for key, value in names.items(): \n",
    "         if val in value: \n",
    "            return key \n",
    "\n",
    "# X list - names\n",
    "x_list = []\n",
    "for key in names.keys():\n",
    "    x_list.extend(names[key])\n",
    "x_list = np.array(x_list)\n",
    "    \n",
    "# Y list - categories\n",
    "y_list = []\n",
    "for name in x_list:\n",
    "    y_list.append(get_key(name))\n",
    "y_list = np.array(y_list)\n",
    "    \n",
    "#length of the longest sequence\n",
    "max_len = len(x_list[0])\n",
    "for names in x_list:\n",
    "    if len(names) > max_len:  \n",
    "        max_len = len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Names into Tensors\n",
    "# --------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryToTensor(category):\n",
    "    category_tensor = torch.tensor([languages.index(category)], dtype=torch.long)\n",
    "    return category_tensor\n",
    "    \n",
    "sequences_name = []\n",
    "for name in x_list:\n",
    "    sequences_name.append(nameToTensor(name))\n",
    "feature = pad_sequence(sequences_name, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "sequences_category = []\n",
    "for category in y_list:\n",
    "    sequences_category.append(categoryToTensor(category))\n",
    "target = pad_sequence(sequences_category, batch_first=True, padding_value=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(         \n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 128,         # number of hidden units\n",
    "            num_layers = 1,           # number of layers\n",
    "            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your RNN will output a tensor with shape (seq_len, batch.\n",
    "            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your RNN will output a tensor with shape (batch_size, seq_len, hidden_size).\n",
    "        )\n",
    "        self.out = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        # r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "        r_out, h = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose last time step of output\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for easy mini-batch return in training\n",
    "import torch.utils.data as data_utils\n",
    "train_data = data_utils.TensorDataset(feature, target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.46861612035468764\n"
     ]
    }
   ],
   "source": [
    "# %% training and testing\n",
    "for epoch in range(NEPOCHS):\n",
    "    for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "        #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "        b_x = x\n",
    "        b_y = y                                         # batch y\n",
    "\n",
    "        output = rnn(b_x)                               # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "# getting accuracy        \n",
    "n_correct = 0\n",
    "for i in range(0, len(sequences_name)):\n",
    "    test_output = rnn(sequences_name[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    #import pdb;pdb.set_trace()\n",
    "    if pred_y == sequences_category[i].data.numpy().squeeze():\n",
    "        n_correct += 1\n",
    "accuracy = n_correct / len(sequences_name)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.Modify the implementation from 1.1 to support arbitrary mini-batch sizes. In this case, instead of padding to a unique sequence length, adaptively pad the length of the mini batch to the length of the longest sample in the mini batch itself. Report the accuracy number (on the full training set) yielded by this approach on mini batch sizes of 1000, 2000 and 5000 after five epochs of training. (10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runned the same code 3 times by only changing the hidden states sizes to 32, 64 and 128\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Hyper Parameters\n",
    "NEPOCHS = 5\n",
    "BATCH_SIZE = 1000\n",
    "TIME_STEP = 57          \n",
    "INPUT_SIZE = 57         \n",
    "LR = 0.01               # learning rate\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the names dictionary, a list of names per language\n",
    "# dictionary keys are languages, values are names\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def findName(dict, name):\n",
    "    keys = dict.keys()\n",
    "    for key in keys:\n",
    "        if name in dict[key]:\n",
    "            return key\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "def get_key(val): \n",
    "    for key, value in names.items(): \n",
    "         if val in value: \n",
    "            return key \n",
    "\n",
    "# X list - names\n",
    "x_list = []\n",
    "for key in names.keys():\n",
    "    x_list.extend(names[key])\n",
    "x_list = np.array(x_list)\n",
    "    \n",
    "# Y list - categories\n",
    "y_list = []\n",
    "for name in x_list:\n",
    "    y_list.append(get_key(name))\n",
    "y_list = np.array(y_list)\n",
    "    \n",
    "#length of the longest sequence\n",
    "max_len = len(x_list[0])\n",
    "for names in x_list:\n",
    "    if len(names) > max_len:  \n",
    "        max_len = len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Names into Tensors\n",
    "# --------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryToTensor(category):\n",
    "    category_tensor = torch.tensor([languages.index(category)], dtype=torch.long)\n",
    "    return category_tensor\n",
    "    \n",
    "sequences_name = []\n",
    "for name in x_list:\n",
    "    sequences_name.append(nameToTensor(name))\n",
    "feature = pad_sequence(sequences_name, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "sequences_category = []\n",
    "for category in y_list:\n",
    "    sequences_category.append(categoryToTensor(category))\n",
    "target = pad_sequence(sequences_category, batch_first=True, padding_value=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(         \n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 128,         # number of hidden units\n",
    "            num_layers = 1,           # number of layers\n",
    "            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your RNN will output a tensor with shape (seq_len, batch.\n",
    "            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your RNN will output a tensor with shape (batch_size, seq_len, hidden_size).\n",
    "        )\n",
    "        self.out = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        # r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "        r_out, h = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose last time step of output\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for easy mini-batch return in training\n",
    "import torch.utils.data as data_utils\n",
    "train_data = data_utils.TensorDataset(feature, target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3248978778519478\n"
     ]
    }
   ],
   "source": [
    "# %% training and testing\n",
    "for epoch in range(NEPOCHS):\n",
    "    for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "        #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "        b_x = x\n",
    "        b_y = y                                         # batch y\n",
    "\n",
    "        output = rnn(b_x)                               # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "# getting accuracy        \n",
    "n_correct = 0\n",
    "for i in range(0, len(sequences_name)):\n",
    "    test_output = rnn(sequences_name[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    #import pdb;pdb.set_trace()\n",
    "    if pred_y == sequences_category[i].data.numpy().squeeze():\n",
    "        n_correct += 1\n",
    "accuracy = n_correct / len(sequences_name)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model cross-validation (20 + 15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.Modify the implementation from 1.1 or 1.2 to enable support of five-fold cross-validation by leveraging the Kfold object from scikitlearn. Report the average accuracy on the full training and test sets across all five folds for two cases: (a) after 5 epochs of training, and (b) after however many epochs it takes the algorithm to converge. (10 + 5 points). For (b), feel free to use a batch size of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runned the same code 3 times by only changing the hidden states sizes to 32, 64 and 128\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Hyper Parameters\n",
    "NEPOCHS = 5\n",
    "BATCH_SIZE = 20074\n",
    "TIME_STEP = 57          \n",
    "INPUT_SIZE = 57         \n",
    "LR = 0.001               # learning rate\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the names dictionary, a list of names per language\n",
    "# dictionary keys are languages, values are names\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def findName(dict, name):\n",
    "    keys = dict.keys()\n",
    "    for key in keys:\n",
    "        if name in dict[key]:\n",
    "            return key\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "import numpy as np\n",
    "\n",
    "def get_key(val): \n",
    "    for key, value in names.items(): \n",
    "         if val in value: \n",
    "            return key \n",
    "\n",
    "# X list - names\n",
    "x_list = []\n",
    "for key in names.keys():\n",
    "    x_list.extend(names[key])\n",
    "x_list = np.array(x_list)\n",
    "    \n",
    "# Y list - categories\n",
    "y_list = []\n",
    "for name in x_list:\n",
    "    y_list.append(get_key(name))\n",
    "y_list = np.array(y_list)\n",
    "    \n",
    "#length of the longest sequence\n",
    "max_len = len(x_list[0])\n",
    "for names in x_list:\n",
    "    if len(names) > max_len:  \n",
    "        max_len = len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Names into Tensors\n",
    "# --------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryToTensor(category):\n",
    "    category_tensor = torch.tensor([languages.index(category)], dtype=torch.long)\n",
    "    return category_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(         \n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 128,         # number of hidden units\n",
    "            num_layers = 1,           # number of layers\n",
    "            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your RNN will output a tensor with shape (seq_len, batch.\n",
    "            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your RNN will output a tensor with shape (batch_size, seq_len, hidden_size).\n",
    "        )\n",
    "        self.out = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        # r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "        r_out, h = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose last time step of output\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy on training set:  0.4674014571268448\n",
      "Accuracy on test set:  0.4734744707347447\n",
      "Fold 2\n",
      "Accuracy on training set:  0.4703281648919609\n",
      "Accuracy on test set:  0.46176836861768367\n",
      "Fold 3\n",
      "Accuracy on training set:  0.4656578865433713\n",
      "Accuracy on test set:  0.4804483188044832\n",
      "Fold 4\n",
      "Accuracy on training set:  0.4682109720406003\n",
      "Accuracy on test set:  0.4702366127023661\n",
      "Fold 5\n",
      "Accuracy on training set:  0.4714819427148194\n",
      "Accuracy on test set:  0.4571499750871948\n",
      "Average accuracy on full training:  0.46861608466351934\n",
      "Average accuracy on test sets:  0.4686155491892945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "from statistics import mean\n",
    "\n",
    "# Kfold five-vold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle = True)\n",
    "training_accuracy_list = []\n",
    "testing_accuracy_list = []\n",
    "# fold number\n",
    "f = 1\n",
    "\n",
    "for train, test in kfold.split(x_list):\n",
    "    rnn = RNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = x_list[train], x_list[test], y_list[train], y_list[test]\n",
    "    \n",
    "    x_train_tensor = []\n",
    "    for name in x_train:\n",
    "        x_train_tensor.append(nameToTensor(name))\n",
    "    feature = pad_sequence(x_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_train_tensor = []\n",
    "    for category in y_train:\n",
    "        y_train_tensor.append(categoryToTensor(category))\n",
    "    target = pad_sequence(y_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    x_test_tensor = []\n",
    "    for name in x_test:\n",
    "        x_test_tensor.append(nameToTensor(name))\n",
    "    #x_test = pad_sequence(x_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_test_tensor = []\n",
    "    for category in y_test:\n",
    "        y_test_tensor.append(categoryToTensor(category))\n",
    "    #y_test = pad_sequence(y_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    #Data Loader for easy mini-batch return in training\n",
    "    import torch.utils.data as data_utils\n",
    "    train_data = data_utils.TensorDataset(feature, target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # %% training and testing\n",
    "    print(\"Fold\",f)\n",
    "    f += 1\n",
    "    for epoch in range(NEPOCHS):\n",
    "        for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "            #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "            b_x = x\n",
    "            b_y = y                                         # batch y\n",
    "\n",
    "            output = rnn(b_x)                               # rnn output\n",
    "            loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "            optimizer.zero_grad()                           # clear gradients for this training step\n",
    "            loss.backward()                                 # backpropagation, compute gradients\n",
    "            optimizer.step()                                # apply gradients\n",
    "    \n",
    "    # getting accuracy of training set        \n",
    "    n_correct1 = 0\n",
    "    for i in range(0, len(x_train_tensor)):\n",
    "        test_output = rnn(x_train_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_train_tensor[i].data.numpy().squeeze():\n",
    "            n_correct1 += 1\n",
    "    accuracy = n_correct1 / len(x_train_tensor)\n",
    "    print(\"Accuracy on training set: \", accuracy)\n",
    "    training_accuracy_list.append(accuracy)\n",
    "    \n",
    "    # getting accuracy of test set        \n",
    "    n_correct2 = 0\n",
    "    for i in range(0, len(x_test_tensor)):\n",
    "        test_output = rnn(x_test_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_test_tensor[i].data.numpy().squeeze():\n",
    "            n_correct2 += 1\n",
    "    accuracy = n_correct2 / len(x_test_tensor)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "    testing_accuracy_list.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy on full training: \", mean(training_accuracy_list))\n",
    "print(\"Average accuracy on test sets: \", mean(testing_accuracy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy on training set:  0.4692072980882994\n",
      "Accuracy on test set:  0.46625155666251555\n",
      "Fold 2\n",
      "Accuracy on training set:  0.46609377918923967\n",
      "Accuracy on test set:  0.47870485678704855\n",
      "Fold 3\n",
      "Accuracy on training set:  0.4669032941029952\n",
      "Accuracy on test set:  0.47546699875466997\n",
      "Fold 4\n",
      "Accuracy on training set:  0.47001681300205495\n",
      "Accuracy on test set:  0.46301369863013697\n",
      "Fold 5\n",
      "Accuracy on training set:  0.47085927770859276\n",
      "Accuracy on test set:  0.45964125560538116\n",
      "Average accuracy on full training:  0.4686160924182364\n",
      "Average accuracy on test sets:  0.46861567328795045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "from statistics import mean\n",
    "\n",
    "# Kfold five-vold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle = True)\n",
    "training_accuracy_list = []\n",
    "testing_accuracy_list = []\n",
    "# fold number\n",
    "f = 1\n",
    "\n",
    "for train, test in kfold.split(x_list):\n",
    "    rnn = RNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = x_list[train], x_list[test], y_list[train], y_list[test]\n",
    "    \n",
    "    x_train_tensor = []\n",
    "    for name in x_train:\n",
    "        x_train_tensor.append(nameToTensor(name))\n",
    "    feature = pad_sequence(x_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_train_tensor = []\n",
    "    for category in y_train:\n",
    "        y_train_tensor.append(categoryToTensor(category))\n",
    "    target = pad_sequence(y_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    x_test_tensor = []\n",
    "    for name in x_test:\n",
    "        x_test_tensor.append(nameToTensor(name))\n",
    "    #x_test = pad_sequence(x_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_test_tensor = []\n",
    "    for category in y_test:\n",
    "        y_test_tensor.append(categoryToTensor(category))\n",
    "    #y_test = pad_sequence(y_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    #Data Loader for easy mini-batch return in training\n",
    "    import torch.utils.data as data_utils\n",
    "    train_data = data_utils.TensorDataset(feature, target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # %% training and testing\n",
    "    print(\"Fold\",f)\n",
    "    f += 1\n",
    "    NEPOCHS = 20\n",
    "    for epoch in range(NEPOCHS):\n",
    "        for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "            #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "            b_x = x\n",
    "            b_y = y                                         # batch y\n",
    "\n",
    "            output = rnn(b_x)                               # rnn output\n",
    "            loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "            optimizer.zero_grad()                           # clear gradients for this training step\n",
    "            loss.backward()                                 # backpropagation, compute gradients\n",
    "            optimizer.step()                                # apply gradients\n",
    "    \n",
    "    # getting accuracy of training set        \n",
    "    n_correct1 = 0\n",
    "    for i in range(0, len(x_train_tensor)):\n",
    "        test_output = rnn(x_train_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_train_tensor[i].data.numpy().squeeze():\n",
    "            n_correct1 += 1\n",
    "    accuracy = n_correct1 / len(x_train_tensor)\n",
    "    print(\"Accuracy on training set: \", accuracy)\n",
    "    training_accuracy_list.append(accuracy)\n",
    "    \n",
    "    # getting accuracy of test set        \n",
    "    n_correct2 = 0\n",
    "    for i in range(0, len(x_test_tensor)):\n",
    "        test_output = rnn(x_test_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_test_tensor[i].data.numpy().squeeze():\n",
    "            n_correct2 += 1\n",
    "    accuracy = n_correct2 / len(x_test_tensor)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "    testing_accuracy_list.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy on full training: \", mean(training_accuracy_list))\n",
    "print(\"Average accuracy on test sets: \", mean(testing_accuracy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.Modify the implementation from 2.1 to incorporate the LSTM subclass of module torch.nn. Report the average accuracy on the full training and test sets across all five folds for two cases: (a) after 5 epochs of training, and (b) after however many epochs it takes the algorithm to converge. (10 + 5 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runned the same code 3 times by only changing the hidden states sizes to 32, 64 and 128\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Hyper Parameters\n",
    "NEPOCHS = 5\n",
    "BATCH_SIZE = 20074\n",
    "TIME_STEP = 57          \n",
    "INPUT_SIZE = 57         \n",
    "LR = 0.01               # learning rate\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the names dictionary, a list of names per language\n",
    "# dictionary keys are languages, values are names\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def findName(dict, name):\n",
    "    keys = dict.keys()\n",
    "    for key in keys:\n",
    "        if name in dict[key]:\n",
    "            return key\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "def get_key(val): \n",
    "    for key, value in names.items(): \n",
    "         if val in value: \n",
    "            return key \n",
    "\n",
    "# X list - names\n",
    "x_list = []\n",
    "for key in names.keys():\n",
    "    x_list.extend(names[key])\n",
    "x_list = np.array(x_list)\n",
    "    \n",
    "# Y list - categories\n",
    "y_list = []\n",
    "for name in x_list:\n",
    "    y_list.append(get_key(name))\n",
    "y_list = np.array(y_list)\n",
    "    \n",
    "#length of the longest sequence\n",
    "max_len = len(x_list[0])\n",
    "for names in x_list:\n",
    "    if len(names) > max_len:  \n",
    "        max_len = len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Names into Tensors\n",
    "# --------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryToTensor(category):\n",
    "    category_tensor = torch.tensor([languages.index(category)], dtype=torch.long)\n",
    "    return category_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         \n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 128,         # number of hidden units\n",
    "            num_layers = 1,           # number of layers\n",
    "            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your LSTM will give output of shape (seq_len, batch.\n",
    "            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your LSTM will give output of shape (batch_size, seq_len, hidden_size).\n",
    "        )\n",
    "        self.out = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose last time step of r_out\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy on training set:  0.469394109222243\n",
      "Accuracy on test set:  0.4655043586550436\n",
      "Fold 2\n",
      "Accuracy on training set:  0.4706395167818669\n",
      "Accuracy on test set:  0.46052303860523036\n",
      "Fold 3\n",
      "Accuracy on training set:  0.4650351827635594\n",
      "Accuracy on test set:  0.4829389788293898\n",
      "Fold 4\n",
      "Accuracy on training set:  0.46852232393050625\n",
      "Accuracy on test set:  0.4689912826899128\n",
      "Fold 5\n",
      "Accuracy on training set:  0.46948941469489414\n",
      "Accuracy on test set:  0.46512207274539114\n",
      "Average accuracy on full training:  0.46861610947861393\n",
      "Average accuracy on test sets:  0.46861594630499354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "from statistics import mean\n",
    "\n",
    "# Kfold five-vold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle = True)\n",
    "training_accuracy_list = []\n",
    "testing_accuracy_list = []\n",
    "# fold number\n",
    "f = 1\n",
    "\n",
    "for train, test in kfold.split(x_list):\n",
    "    rnn = RNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = x_list[train], x_list[test], y_list[train], y_list[test]\n",
    "    \n",
    "    x_train_tensor = []\n",
    "    for name in x_train:\n",
    "        x_train_tensor.append(nameToTensor(name))\n",
    "    feature = pad_sequence(x_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_train_tensor = []\n",
    "    for category in y_train:\n",
    "        y_train_tensor.append(categoryToTensor(category))\n",
    "    target = pad_sequence(y_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    x_test_tensor = []\n",
    "    for name in x_test:\n",
    "        x_test_tensor.append(nameToTensor(name))\n",
    "    #x_test = pad_sequence(x_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_test_tensor = []\n",
    "    for category in y_test:\n",
    "        y_test_tensor.append(categoryToTensor(category))\n",
    "    #y_test = pad_sequence(y_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    #Data Loader for easy mini-batch return in training\n",
    "    import torch.utils.data as data_utils\n",
    "    train_data = data_utils.TensorDataset(feature, target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # %% training and testing\n",
    "    print(\"Fold\",f)\n",
    "    f += 1\n",
    "    for epoch in range(NEPOCHS):\n",
    "        for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "            #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "            b_x = x\n",
    "            b_y = y                                         # batch y\n",
    "\n",
    "            output = rnn(b_x)                               # rnn output\n",
    "            loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "            optimizer.zero_grad()                           # clear gradients for this training step\n",
    "            loss.backward()                                 # backpropagation, compute gradients\n",
    "            optimizer.step()                                # apply gradients\n",
    "    \n",
    "    # getting accuracy of training set        \n",
    "    n_correct1 = 0\n",
    "    for i in range(0, len(x_train_tensor)):\n",
    "        test_output = rnn(x_train_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_train_tensor[i].data.numpy().squeeze():\n",
    "            n_correct1 += 1\n",
    "    accuracy = n_correct1 / len(x_train_tensor)\n",
    "    print(\"Accuracy on training set: \", accuracy)\n",
    "                #accuracy = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "                #print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)\n",
    "    training_accuracy_list.append(accuracy)\n",
    "    \n",
    "    # getting accuracy of test set        \n",
    "    n_correct2 = 0\n",
    "    for i in range(0, len(x_test_tensor)):\n",
    "        test_output = rnn(x_test_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_test_tensor[i].data.numpy().squeeze():\n",
    "            n_correct2 += 1\n",
    "    accuracy = n_correct2 / len(x_test_tensor)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "                #accuracy = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "                #print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)\n",
    "    testing_accuracy_list.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy on full training: \", mean(training_accuracy_list))\n",
    "print(\"Average accuracy on test sets: \", mean(testing_accuracy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy on training set:  0.47088859829379165\n",
      "Accuracy on test set:  0.45952677459526775\n",
      "Fold 2\n",
      "Accuracy on training set:  0.46677875334703284\n",
      "Accuracy on test set:  0.4759651307596513\n",
      "Fold 3\n",
      "Accuracy on training set:  0.4689582165763746\n",
      "Accuracy on test set:  0.4672478206724782\n",
      "Fold 4\n",
      "Accuracy on training set:  0.4678996201506943\n",
      "Accuracy on test set:  0.4714819427148194\n",
      "Fold 5\n",
      "Accuracy on training set:  0.46855541718555416\n",
      "Accuracy on test set:  0.46885899352267063\n",
      "Average accuracy on full training:  0.4686161211106895\n",
      "Average accuracy on test sets:  0.46861613245297745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "from statistics import mean\n",
    "\n",
    "# Kfold five-vold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle = True)\n",
    "training_accuracy_list = []\n",
    "testing_accuracy_list = []\n",
    "# fold number\n",
    "f = 1\n",
    "\n",
    "for train, test in kfold.split(x_list):\n",
    "    rnn = RNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = x_list[train], x_list[test], y_list[train], y_list[test]\n",
    "    \n",
    "    x_train_tensor = []\n",
    "    for name in x_train:\n",
    "        x_train_tensor.append(nameToTensor(name))\n",
    "    feature = pad_sequence(x_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_train_tensor = []\n",
    "    for category in y_train:\n",
    "        y_train_tensor.append(categoryToTensor(category))\n",
    "    target = pad_sequence(y_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    x_test_tensor = []\n",
    "    for name in x_test:\n",
    "        x_test_tensor.append(nameToTensor(name))\n",
    "    #x_test = pad_sequence(x_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_test_tensor = []\n",
    "    for category in y_test:\n",
    "        y_test_tensor.append(categoryToTensor(category))\n",
    "    #y_test = pad_sequence(y_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    #Data Loader for easy mini-batch return in training\n",
    "    import torch.utils.data as data_utils\n",
    "    train_data = data_utils.TensorDataset(feature, target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # %% training and testing\n",
    "    print(\"Fold\",f)\n",
    "    f += 1\n",
    "    NEPOCHS = 30\n",
    "    for epoch in range(NEPOCHS):\n",
    "        for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "            #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "            b_x = x\n",
    "            b_y = y                                         # batch y\n",
    "\n",
    "            output = rnn(b_x)                               # rnn output\n",
    "            loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "            optimizer.zero_grad()                           # clear gradients for this training step\n",
    "            loss.backward()                                 # backpropagation, compute gradients\n",
    "            optimizer.step()                                # apply gradients\n",
    "    \n",
    "    # getting accuracy of training set        \n",
    "    n_correct1 = 0\n",
    "    for i in range(0, len(x_train_tensor)):\n",
    "        test_output = rnn(x_train_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_train_tensor[i].data.numpy().squeeze():\n",
    "            n_correct1 += 1\n",
    "    accuracy = n_correct1 / len(x_train_tensor)\n",
    "    print(\"Accuracy on training set: \", accuracy)\n",
    "                #accuracy = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "                #print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)\n",
    "    training_accuracy_list.append(accuracy)\n",
    "    \n",
    "    # getting accuracy of test set        \n",
    "    n_correct2 = 0\n",
    "    for i in range(0, len(x_test_tensor)):\n",
    "        test_output = rnn(x_test_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_test_tensor[i].data.numpy().squeeze():\n",
    "            n_correct2 += 1\n",
    "    accuracy = n_correct2 / len(x_test_tensor)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "                #accuracy = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "                #print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)\n",
    "    testing_accuracy_list.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy on full training: \", mean(training_accuracy_list))\n",
    "print(\"Average accuracy on test sets: \", mean(testing_accuracy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.Modify the implementation from 2.2 to enable support of stratified five- fold cross-validation by leveraging the StratifiedKFold object from scikitlearn. Report the average accuracy on the full training and test sets across all five folds after 5 epochs of training. Feel free to use a batch size of your choosing. (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runned the same code 3 times by only changing the hidden states sizes to 32, 64 and 128\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Hyper Parameters\n",
    "NEPOCHS = 5\n",
    "BATCH_SIZE = 20074\n",
    "TIME_STEP = 57          \n",
    "INPUT_SIZE = 57         \n",
    "LR = 0.01               # learning rate\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the names dictionary, a list of names per language\n",
    "# dictionary keys are languages, values are names\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def findName(dict, name):\n",
    "    keys = dict.keys()\n",
    "    for key in keys:\n",
    "        if name in dict[key]:\n",
    "            return key\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "def get_key(val): \n",
    "    for key, value in names.items(): \n",
    "         if val in value: \n",
    "            return key \n",
    "\n",
    "# X list - names\n",
    "x_list = []\n",
    "for key in names.keys():\n",
    "    x_list.extend(names[key])\n",
    "x_list = np.array(x_list)\n",
    "    \n",
    "# Y list - categories\n",
    "y_list = []\n",
    "for name in x_list:\n",
    "    y_list.append(get_key(name))\n",
    "y_list = np.array(y_list)\n",
    "    \n",
    "#length of the longest sequence\n",
    "max_len = len(x_list[0])\n",
    "for names in x_list:\n",
    "    if len(names) > max_len:  \n",
    "        max_len = len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Names into Tensors\n",
    "# --------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryToTensor(category):\n",
    "    category_tensor = torch.tensor([languages.index(category)], dtype=torch.long)\n",
    "    return category_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         \n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 128,         # number of hidden units\n",
    "            num_layers = 1,           # number of layers\n",
    "            batch_first = True,       # If your input data is of shape (seq_len, batch_size, features) then you donâ€™t need batch_first=True and your LSTM will give output of shape (seq_len, batch.\n",
    "            #If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your LSTM will give output of shape (batch_size, seq_len, hidden_size).\n",
    "        )\n",
    "        self.out = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose last time step of r_out\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jungmin/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy on training set:  0.46878893595813603\n",
      "Accuracy on test set:  0.4679264047737444\n",
      "Fold 2\n",
      "Accuracy on training set:  0.4686721474838067\n",
      "Accuracy on test set:  0.4683922349427576\n",
      "Fold 3\n",
      "Accuracy on training set:  0.4685885063196563\n",
      "Accuracy on test set:  0.46872663842511836\n",
      "Fold 4\n",
      "Accuracy on training set:  0.46855933258622834\n",
      "Accuracy on test set:  0.4688434695912263\n",
      "Fold 5\n",
      "Accuracy on training set:  0.4684718331777155\n",
      "Accuracy on test set:  0.46919431279620855\n",
      "Average accuracy on full training:  0.46861615110510857\n",
      "Average accuracy on test sets:  0.46861661210581107\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "\n",
    "# Stratified Kfold five-vold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "training_accuracy_list = []\n",
    "testing_accuracy_list = []\n",
    "# fold number\n",
    "f = 1\n",
    "\n",
    "for train, test in skf.split(x_list, y_list):\n",
    "    rnn = RNN()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = x_list[train], x_list[test], y_list[train], y_list[test]\n",
    "    \n",
    "    x_train_tensor = []\n",
    "    for name in x_train:\n",
    "        x_train_tensor.append(nameToTensor(name))\n",
    "    feature = pad_sequence(x_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_train_tensor = []\n",
    "    for category in y_train:\n",
    "        y_train_tensor.append(categoryToTensor(category))\n",
    "    target = pad_sequence(y_train_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    x_test_tensor = []\n",
    "    for name in x_test:\n",
    "        x_test_tensor.append(nameToTensor(name))\n",
    "    #x_test = pad_sequence(x_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "\n",
    "    y_test_tensor = []\n",
    "    for category in y_test:\n",
    "        y_test_tensor.append(categoryToTensor(category))\n",
    "    #y_test = pad_sequence(y_test_tensor, batch_first=True, padding_value=0).squeeze()\n",
    "    \n",
    "    #Data Loader for easy mini-batch return in training\n",
    "    import torch.utils.data as data_utils\n",
    "    train_data = data_utils.TensorDataset(feature, target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # %% training and testing\n",
    "    print(\"Fold\",f)\n",
    "    f += 1\n",
    "    for epoch in range(NEPOCHS):\n",
    "        for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "            #b_x = x.view(-1, 19, 19)                        # reshape x to (batch, time_step, input_size)\n",
    "            b_x = x\n",
    "            b_y = y                                         # batch y\n",
    "\n",
    "            output = rnn(b_x)                               # rnn output\n",
    "            loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "            optimizer.zero_grad()                           # clear gradients for this training step\n",
    "            loss.backward()                                 # backpropagation, compute gradients\n",
    "            optimizer.step()                                # apply gradients\n",
    "    \n",
    "    # getting accuracy of training set        \n",
    "    n_correct1 = 0\n",
    "    for i in range(0, len(x_train_tensor)):\n",
    "        test_output = rnn(x_train_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_train_tensor[i].data.numpy().squeeze():\n",
    "            n_correct1 += 1\n",
    "    accuracy = n_correct1 / len(x_train_tensor)\n",
    "    print(\"Accuracy on training set: \", accuracy)\n",
    "                #accuracy = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "                #print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)\n",
    "    training_accuracy_list.append(accuracy)\n",
    "    \n",
    "    # getting accuracy of test set        \n",
    "    n_correct2 = 0\n",
    "    for i in range(0, len(x_test_tensor)):\n",
    "        test_output = rnn(x_test_tensor[i].transpose(0, 1))                   # (samples, time_step, input_size)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if pred_y == y_test_tensor[i].data.numpy().squeeze():\n",
    "            n_correct2 += 1\n",
    "    accuracy = n_correct2 / len(x_test_tensor)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "                #accuracy = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "                #print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %.2f' % accuracy)\n",
    "    testing_accuracy_list.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy on full training: \", mean(training_accuracy_list))\n",
    "print(\"Average accuracy on test sets: \", mean(testing_accuracy_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
